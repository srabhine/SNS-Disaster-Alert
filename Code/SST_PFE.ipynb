{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import pandas as pd\n",
    " \n",
    "# assign the values accordingly\n",
    "consumer_key = \"vILOcHxSRk9zOy5fZH3x7kIjS\"\n",
    "consumer_secret = \"VzPDu5BEq3x8VglxJPGKa1lFKfJ5OoKZfd90KOES1mdtMQsezo\"\n",
    "access_token = \"1526192300182978562-iHYctxjSI0k1cXZGIihYu6jQQJYZpF\"\n",
    "access_token_secret = \"zSwpEAjQwpDFL6jzhSrZZTf3spepUAxzTbvwy8TfNT1g9\"\n",
    "\n",
    "# authorization of consumer key and consumer secret\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    " \n",
    "# set access to user's access key and access secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    " \n",
    "# calling the api\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAJm%2BcgEAAAAAdT6oZA5nHHlM%2BU07cRioA8QSXvI%3D1ZN1biqO1uHYO4GxiOEpSXCTElUu7fz7fjk8GTmEus6DJBYp2G',wait_on_rate_limit=True)\n",
    "\n",
    "    \n",
    "retweets_list=[]\n",
    "like_list=[]\n",
    "user_listlike=[]\n",
    "user_listRT=[]\n",
    "\n",
    "tweet_id=\"1518972397277396992\"\n",
    "\n",
    "    #get received retweets\n",
    "for user in tweepy.Paginator(client.get_retweeters,id=tweet_id, max_results=100).flatten(limit=100): \n",
    "    retweets_list.append(user.username)\n",
    "time.sleep(20)\n",
    "    #get received likes\n",
    "for user in tweepy.Paginator(client.get_liking_users,id=tweet_id, max_results=100).flatten(limit=100):\n",
    "    like_list.append(user.username)\n",
    "time.sleep(20)\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "retweets_list = list(set(retweets_list))\n",
    "like_list=list(set(like_list))\n",
    "dfRT = pd.DataFrame(retweets_list, columns=['Username'])\n",
    "dfL= pd.DataFrame(like_list, columns=['Username'])\n",
    "#dfRT.to_csv('C:/Users/srabh/Documents/PFEcode/SSTRT.csv')\n",
    "#dfL.to_csv('C:/Users/srabh/Documents/PFEcode/SSTLikes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#telecharger entre 2 dates avant le Newom (2 semaines avant) les tweets du mec declencheur\n",
    "#regarder si dans la liste qu'on a telecharger si elles ont interagit(liker,RT) les tweets des 2 semaines d'avant\n",
    "\n",
    "SST_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-03-01T00:00:00.000Z\n"
     ]
    }
   ],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time\n",
    "\n",
    "\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAJm%2BcgEAAAAAdT6oZA5nHHlM%2BU07cRioA8QSXvI%3D1ZN1biqO1uHYO4GxiOEpSXCTElUu7fz7fjk8GTmEus6DJBYp2G'\n",
    "\n",
    "def auth(): \n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "\n",
    "def create_headers(bearer_token): \n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "#def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    #params['next_token'] = next_token   #params object received from create_url function\n",
    "    #response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    #print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    #if response.status_code != 200:\n",
    "        #raise Exception(response.status_code, response.text)\n",
    "    #return response.json()\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    headers = create_headers(auth())\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    \n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']#['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "        \n",
    "        if ('in_reply_to_user_id' in tweet):\n",
    "            in_reply_to_user_id = tweet['in_reply_to_user_id']\n",
    "        else:\n",
    "            in_reply_to_user_id = \" \"\n",
    "        \n",
    "        \n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "        \n",
    "        #test\n",
    "        conversation_id= tweet['conversation_id']\n",
    "        \n",
    "      \n",
    "\n",
    "\n",
    "        # 7. source\n",
    "        #source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count,conversation_id,in_reply_to_user_id,text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) \n",
    "    \n",
    "# append_to_csv(json_response, \"data.csv\") \n",
    "\n",
    "def tweets_to_dataframe(json_response):\n",
    "    tweets = []\n",
    "    for tweet in json_response['data']:\n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']#['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "        \n",
    "        if ('in_reply_to_user_id' in tweet):\n",
    "            in_reply_to_user_id = tweet['in_reply_to_user_id']\n",
    "        else:\n",
    "            in_reply_to_user_id = \" \"\n",
    "        \n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "        \n",
    "        #test\n",
    "        conversation_id= tweet['conversation_id']\n",
    "        \n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a dictionary\n",
    "        tweets.append({'author_id': author_id, 'created_at': created_at, 'geo': geo, 'tweet_id': tweet_id, 'lang': lang, 'like_count': like_count, 'quote_count': quote_count, 'reply_count': reply_count, 'retweet_count': retweet_count,'conversation_id':conversation_id,'in_reply_to_user_id':in_reply_to_user_id,'text':text})\n",
    "    # Return the tweets as a dataframe\n",
    "    return pd.DataFrame(tweets)\n",
    "    \n",
    "bearer_token='AAAAAAAAAAAAAAAAAAAAAJm%2BcgEAAAAAdT6oZA5nHHlM%2BU07cRioA8QSXvI%3D1ZN1biqO1uHYO4GxiOEpSXCTElUu7fz7fjk8GTmEus6DJBYp2G'\n",
    "#Inputs for tweets\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword=[\"from:819884626328121348\"]\n",
    "start_list=[\"2022-03-01T00:00:00.000Z\"]\n",
    "end_list=[\"2022-04-03T00:00:00.000Z\"]\n",
    "lang='en'\n",
    "max_results = 500\n",
    "\n",
    "total_tweets = 0\n",
    "\n",
    "# Create file\n",
    "csvFile = open(\"SSTdelta.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','conversation_id','in_reply_to_user_id','tweet'])\n",
    "csvFile.close()\n",
    "for j in range(len(keyword)):\n",
    "    res=keyword[j]\n",
    "    for i in range(0,len(start_list)):\n",
    "\n",
    "        num_file=str(j)\n",
    "\n",
    "        # Inputs\n",
    "        count = 0 # Counting tweets per time period\n",
    "        max_count = 2000000 # Max tweets per time period\n",
    "        flag = True\n",
    "        next_token = None\n",
    "        # Check if flag is true\n",
    "        while flag:\n",
    "            # Check if max_count reached\n",
    "            if count >= max_count:\n",
    "                break\n",
    "            print(\"-------------------\")\n",
    "            print(\"Token: \", next_token)\n",
    "            url = create_url(res, start_list[i],end_list[i], max_results)\n",
    "            json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "            result_count = json_response['meta']['result_count']\n",
    "\n",
    "            if 'next_token' in json_response['meta']:\n",
    "                # Save the token to use for next call\n",
    "                next_token = json_response['meta']['next_token']\n",
    "                print(\"Next Token: \", next_token)\n",
    "                if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, \"C:/Users/srabh/Documents/PFEcode/SSTdelta.csv\")\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5)                \n",
    "            # If no next token exists\n",
    "            else:\n",
    "                if result_count is not None and result_count > 0:\n",
    "                    print(\"-------------------\")\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    #name_file=\"test_\"+num_file+\".csv\"\n",
    "                    df=tweets_to_dataframe(json_response)\n",
    "                    auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "                    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "                    counter=0\n",
    "                    for k in df.iloc[:,3]:\n",
    "                        tweet = api.get_status(k)\n",
    "                        # Count likes\n",
    "                        counter += tweet.favorite_count\n",
    "                        # Count retweets\n",
    "                        counter += tweet.retweet_count\n",
    "                        if len(df.iloc[:,3])!=0:\n",
    "                            SST=counter/len(df.iloc[:,3])\n",
    "                        else:\n",
    "                            SST=0\n",
    "                    SST_list.append(SST)\n",
    "                    #count += result_count\n",
    "                    #total_tweets += result_count\n",
    "                    #print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    #print(\"-------------------\")\n",
    "                    time.sleep(5)\n",
    "                \n",
    "                #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                flag = False\n",
    "                next_token = None\n",
    "            time.sleep(5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[212.51111111111112,\n",
       " 410.3333333333333,\n",
       " 16.907692307692308,\n",
       " 631.5802469135803,\n",
       " 1155.7619047619048,\n",
       " 1155.7619047619048,\n",
       " 31752.444444444445,\n",
       " 122.24,\n",
       " 78.8021978021978,\n",
       " 1339.3555555555556,\n",
       " 471.7464788732394,\n",
       " 344.4642857142857,\n",
       " 1016.7619047619048,\n",
       " 489.4423076923077,\n",
       " 229.50602409638554,\n",
       " 37.67695961995249]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SST_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-04-22T00:00:00.000Z\n",
      "Total number of results:  0\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Total number of results:  0\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-04-22T00:00:00.000Z\n",
      "Total number of results:  0\n"
     ]
    }
   ],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time\n",
    "\n",
    "\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAJm%2BcgEAAAAAdT6oZA5nHHlM%2BU07cRioA8QSXvI%3D1ZN1biqO1uHYO4GxiOEpSXCTElUu7fz7fjk8GTmEus6DJBYp2G'\n",
    "\n",
    "def auth(): \n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "\n",
    "def create_headers(bearer_token): \n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    \n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']#['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "        \n",
    "        if ('in_reply_to_user_id' in tweet):\n",
    "            in_reply_to_user_id = tweet['in_reply_to_user_id']\n",
    "        else:\n",
    "            in_reply_to_user_id = \" \"\n",
    "        \n",
    "        \n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "        \n",
    "        #test\n",
    "        conversation_id= tweet['conversation_id']\n",
    "        \n",
    "      \n",
    "\n",
    "\n",
    "        # 7. source\n",
    "        #source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count,conversation_id,in_reply_to_user_id,text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) \n",
    "\n",
    "def tweets_to_dataframe(json_response):\n",
    "    tweets = []\n",
    "    for tweet in json_response['data']:\n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']#['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "        \n",
    "        if ('in_reply_to_user_id' in tweet):\n",
    "            in_reply_to_user_id = tweet['in_reply_to_user_id']\n",
    "        else:\n",
    "            in_reply_to_user_id = \" \"\n",
    "        \n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "        \n",
    "        #test\n",
    "        conversation_id= tweet['conversation_id']\n",
    "        \n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a dictionary\n",
    "        tweets.append({'author_id': author_id, 'created_at': created_at, 'geo': geo, 'tweet_id': tweet_id, 'lang': lang, 'like_count': like_count, 'quote_count': quote_count, 'reply_count': reply_count, 'retweet_count': retweet_count,'conversation_id':conversation_id,'in_reply_to_user_id':in_reply_to_user_id,'text':text})\n",
    "    # Return the tweets as a dataframe\n",
    "    return pd.DataFrame(tweets)\n",
    "    \n",
    "# append_to_csv(json_response, \"data.csv\") \n",
    "\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "def previous_datetime(datetime_str):\n",
    "    # Convert the input string to a datetime object\n",
    "    datetime_obj = datetime.datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')\n",
    "    # Subtract 2 days from the input datetime\n",
    "    previous_datetime = datetime_obj - timedelta(days=2)\n",
    "    # Return the previous datetime in the 'YYYY-MM-DD HH:MM:SS' format\n",
    "    return previous_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "from datetime import timedelta\n",
    "def previous_tweet_datetime(created_at):\n",
    "    # code to access the tweet using the tweet_id and retrieve the created_at value\n",
    "    created_at = json_response['created_at']\n",
    "    # Convert the created_at string to a datetime object\n",
    "    datetime_obj = datetime.datetime.strptime(created_at, '%Y-%m-%d %H:%M:%S')\n",
    "    # Subtract 1 second from the created_at datetime\n",
    "    previous_datetime = datetime_obj - timedelta(seconds=1)\n",
    "    # Return the previous datetime in the 'YYYY-MM-DD HH:MM:SS' format\n",
    "    return previous_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "import requests\n",
    "import json\n",
    "consumer_key\n",
    "consumer_secret\n",
    "def get_tweet_creation_date(tweet_id):\n",
    "    # Set the API endpoint URL\n",
    "    url = 'https://api.twitter.com/1.1/statuses/show.json'\n",
    "    # Authenticate with the Twitter API using your API key and secret\n",
    "    api_key=consumer_key\n",
    "    api_secret=consumer_secret \n",
    "    bearer_token = api_key + \":\" + api_secret\n",
    "    headers = {'Authorization': 'Bearer ' + bearer_token}\n",
    "    # Set the parameters for the API request\n",
    "    params = {'id': tweet_id}\n",
    "    # Send the API request\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "\n",
    "#Inputs for tweets\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword=[\"from:elonmusk\",\"from:emmanuelmacron\",\"from:marinelepen\"]\n",
    "start_list=[\"2022-04-22T00:00:00.000Z\"]\n",
    "end_list=[\"2022-04-24T00:00:00.000Z\"] \n",
    "lang='en'\n",
    "max_results = 500\n",
    "\n",
    "total_tweets = 0\n",
    "\n",
    "# Create file\n",
    "csvFile = open(\"SSTdelta.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','conversation_id','in_reply_to_user_id','tweet'])\n",
    "csvFile.close()\n",
    "SST_liste=[]\n",
    "for j in range(len(keyword)):\n",
    "    res=keyword[j]\n",
    "    for i in range(0,len(start_list)):\n",
    "\n",
    "        num_file=str(j)\n",
    "\n",
    "        # Inputs\n",
    "        count = 0 # Counting tweets per time period\n",
    "        max_count = 2000000 # Max tweets per time period\n",
    "        flag = True\n",
    "        next_token = None\n",
    "        # Check if flag is true\n",
    "        while flag:\n",
    "            # Check if max_count reached\n",
    "            if count >= max_count:\n",
    "                break\n",
    "            print(\"-------------------\")\n",
    "            print(\"Token: \", next_token)\n",
    "            url = create_url(res, start_list[i],end_list[i], max_results)\n",
    "            json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "            result_count = json_response['meta']['result_count']\n",
    "\n",
    "            if 'next_token' in json_response['meta']:\n",
    "                # Save the token to use for next call\n",
    "                next_token = json_response['meta']['next_token']\n",
    "                print(\"Next Token: \", next_token)\n",
    "                if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, \"C:/Users/srabh/Documents/PFEcode/SSTdelta.csv\")\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5)                \n",
    "            # If no next token exists\n",
    "            else:\n",
    "                if result_count is not None and result_count > 0:\n",
    "                    print(\"-------------------\")\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    #name_file=\"test_\"+num_file+\".csv\"\n",
    "                    df=tweets_to_dataframe(json_response)\n",
    "                    for k in df.iloc[:,3]:\n",
    "                        counter=0\n",
    "                        tweet = api.get_status(k)\n",
    "                        # Count likes\n",
    "                        counter += tweet.favorite_count\n",
    "                        # Count retweets\n",
    "                        counter += tweet.retweet_count\n",
    "                        if len(df.iloc[:,3])!=0:\n",
    "                            SST=counter/len(df.iloc[:,3])\n",
    "                        else:\n",
    "                            SST=0\n",
    "                    SST_liste.append(SST)\n",
    "                    #count += result_count\n",
    "                    #total_tweets += result_count\n",
    "                    #print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    #print(\"-------------------\")\n",
    "                    time.sleep(5)\n",
    "                \n",
    "                #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                flag = False\n",
    "                next_token = None\n",
    "            time.sleep(5)\n",
    "    print(\"Total number of results: \", total_tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3683.318181818182, 0.5]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SST_liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(name_file)):\n",
    "    #df=pd.read_csv('C:/Users/srabh/AppData/Local/Programs/Microsoft VS Code/test.csv')\n",
    "    df=pd.read_csv('test_'+str(i)+\".csv\")\n",
    "    print(df)\n",
    "    #df.iloc[:,3]\n",
    "\n",
    "    for k in df.iloc[:,3]:\n",
    "        # for user in tweepy.Paginator(client.get_retweeters,id=i, max_results=100).flatten(limit=100): \n",
    "        #     retweets_list_community.append(user.username)\n",
    "        # time.sleep(20)\n",
    "        # #get received likes\n",
    "        # for user in tweepy.Paginator(client.get_liking_users,id=i, max_results=100).flatten(limit=100):\n",
    "        #     like_list_community.append(user.username)\n",
    "        # time.sleep(20)\n",
    "        counter=0\n",
    "        tweet = api.get_status(i)\n",
    "        # Count likes\n",
    "        counter += tweet.favorite_count\n",
    "        # Count retweets\n",
    "        counter += tweet.retweet_count\n",
    "        if len(df.iloc[:,3])!=0:\n",
    "            SST=counter/len(df.iloc[:,3])\n",
    "        else:\n",
    "            SST=0\n",
    "        #return sum(counter[user_id] for user_id in user_id_list)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1283.4545454545455"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_list_community = list(set(retweets_list_community))\n",
    "like_list_community=list(set(like_list_community))\n",
    "\n",
    "#retweets_list\n",
    "sum_like_list=0\n",
    "sum_RT_list=0\n",
    "for i in like_list:\n",
    "    if i in like_list_community or retweets_list_community:\n",
    "        sum_like_list+=1\n",
    "for j in retweets_list:\n",
    "    if j in retweets_list_community or like_list_community:\n",
    "        sum_RT_list+=1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_likes_and_retweets(tweet, user_id_list):\n",
    "    count = 0\n",
    "    user_id_set = set(user_id_list)\n",
    "    for like, retweet in zip(tweet.favorite_count, tweet.retweet_count):\n",
    "        if like.user.id in user_id_set:\n",
    "            count += 1\n",
    "        if retweet.user.id in user_id_set:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_likes_and_retweets(tweet, user_id_list):\n",
    "    counter = Counter()\n",
    "    for like in tweet.favorite_count:\n",
    "        counter[like.user.id] += 1\n",
    "    for retweet in tweet.retweet_count:\n",
    "        counter[retweet.user.id] += 1\n",
    "    return sum(counter[user_id] for user_id in user_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "def count_likes_and_retweets(tweet_id, user_id_list):\n",
    "    counter = Counter()\n",
    "    consumer_key = \"vILOcHxSRk9zOy5fZH3x7kIjS\"\n",
    "    consumer_secret = \"VzPDu5BEq3x8VglxJPGKa1lFKfJ5OoKZfd90KOES1mdtMQsezo\"\n",
    "    access_token = \"1526192300182978562-iHYctxjSI0k1cXZGIihYu6jQQJYZpF\"\n",
    "    access_token_secret = \"zSwpEAjQwpDFL6jzhSrZZTf3spepUAxzTbvwy8TfNT1g9\"\n",
    "    # Authentication\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    # Get the tweet\n",
    "    tweet = api.get_status(tweet_id)\n",
    "    # Count likes\n",
    "    for like in tweet.favorite_count:\n",
    "        counter[like.user.id] += 1\n",
    "    # Count retweets\n",
    "    for retweet in tweet.retweet_count:\n",
    "        counter[retweet.user.id] += 1\n",
    "    return sum(counter[user_id] for user_id in user_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_likes_and_retweets(tweet_id, user_id_list):\n",
    "    counter = Counter()\n",
    "    # Authentication\n",
    "    consumer_key = \"vILOcHxSRk9zOy5fZH3x7kIjS\"\n",
    "    consumer_secret = \"VzPDu5BEq3x8VglxJPGKa1lFKfJ5OoKZfd90KOES1mdtMQsezo\"\n",
    "    access_token = \"1526192300182978562-iHYctxjSI0k1cXZGIihYu6jQQJYZpF\"\n",
    "    access_token_secret = \"zSwpEAjQwpDFL6jzhSrZZTf3spepUAxzTbvwy8TfNT1g9\"\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    # Get the tweet\n",
    "    tweet = api.get_status(tweet_id)\n",
    "    # Count likes\n",
    "    counter[tweet.user.id] += tweet.favorite_count\n",
    "    print(tweet.favorite_count)\n",
    "    # Count retweets\n",
    "    counter[tweet.user.id] += tweet.retweet_count\n",
    "    return counter\n",
    "    #return sum(counter[user_id] for user_id in user_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({1212564056827031553: 489616})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=count_likes_and_retweets(\"1518972397277396992\",retweets_list)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6aa87db21f341d76117af31b973aaa3246ce3aabe9ae0ac2111e3875a6a72f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
